{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7933073,"sourceType":"datasetVersion","datasetId":4663183}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ahmedmzaid/eyeofhourus-classification?scriptVersionId=175669546\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport numpy as np\n%matplotlib inline","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-03-30T09:55:04.780444Z","iopub.execute_input":"2024-03-30T09:55:04.781724Z","iopub.status.idle":"2024-03-30T09:55:05.757973Z","shell.execute_reply.started":"2024-03-30T09:55:04.781682Z","shell.execute_reply":"2024-03-30T09:55:05.757032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function: Create DataFrame\n- Takes a path to a folder (train/val) containing classes (sub-folders)\n- Returns a DataFrame with 2 columns: filename, class","metadata":{}},{"cell_type":"code","source":"def create_dataframe(data_path):\n    df = []\n    for c in os.listdir(data_path):\n        class_folder = os.path.join(data_path, c)\n        for f in os.listdir(class_folder):\n            f_path = os.path.join(class_folder, f)\n            if f_path.endswith(('jpg' , 'png' , 'jpeg', 'bmp')):\n                df.append([f_path, c])\n    return pd.DataFrame(df, columns=('filename', 'class'))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:05.759562Z","iopub.execute_input":"2024-03-30T09:55:05.759961Z","iopub.status.idle":"2024-03-30T09:55:05.766595Z","shell.execute_reply.started":"2024-03-30T09:55:05.759936Z","shell.execute_reply":"2024-03-30T09:55:05.765651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating DataFrames","metadata":{}},{"cell_type":"code","source":"# constants\nIMG_DIM = 224\nDATA_PATH = '/kaggle/input/cctv-gender-classifier-dataset/CCTV Gender Classifier Dataset'\nCLASSES = sorted(['MALE', 'FEMALE'])\nprint(CLASSES)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:05.771378Z","iopub.execute_input":"2024-03-30T09:55:05.771631Z","iopub.status.idle":"2024-03-30T09:55:05.776903Z","shell.execute_reply.started":"2024-03-30T09:55:05.77161Z","shell.execute_reply":"2024-03-30T09:55:05.775995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating dataframes\ndf = create_dataframe(os.path.join(DATA_PATH))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:05.777836Z","iopub.execute_input":"2024-03-30T09:55:05.778108Z","iopub.status.idle":"2024-03-30T09:55:06.752829Z","shell.execute_reply.started":"2024-03-30T09:55:05.778087Z","shell.execute_reply":"2024-03-30T09:55:06.751821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting Data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:13.084794Z","iopub.execute_input":"2024-03-30T09:55:13.085137Z","iopub.status.idle":"2024-03-30T09:55:14.140896Z","shell.execute_reply.started":"2024-03-30T09:55:13.08511Z","shell.execute_reply":"2024-03-30T09:55:14.140051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_val, df_test = train_test_split(df, test_size=0.30, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:14.142394Z","iopub.execute_input":"2024-03-30T09:55:14.1428Z","iopub.status.idle":"2024-03-30T09:55:14.153412Z","shell.execute_reply.started":"2024-03-30T09:55:14.142775Z","shell.execute_reply":"2024-03-30T09:55:14.152598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df_train_val, test_size=0.30, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:14.154627Z","iopub.execute_input":"2024-03-30T09:55:14.155106Z","iopub.status.idle":"2024-03-30T09:55:14.163626Z","shell.execute_reply.started":"2024-03-30T09:55:14.155075Z","shell.execute_reply":"2024-03-30T09:55:14.16293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:16.179313Z","iopub.execute_input":"2024-03-30T09:55:16.179663Z","iopub.status.idle":"2024-03-30T09:55:16.195789Z","shell.execute_reply.started":"2024-03-30T09:55:16.179636Z","shell.execute_reply":"2024-03-30T09:55:16.194839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data generators (reading data from disk)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:19.935767Z","iopub.execute_input":"2024-03-30T09:55:19.936158Z","iopub.status.idle":"2024-03-30T09:55:30.51982Z","shell.execute_reply.started":"2024-03-30T09:55:19.936125Z","shell.execute_reply":"2024-03-30T09:55:30.518963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prefun(image):\n    img = cv2.GaussianBlur(image, (5, 5), 0)\n    return img","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:35.147834Z","iopub.execute_input":"2024-03-30T09:55:35.148818Z","iopub.status.idle":"2024-03-30T09:55:35.154445Z","shell.execute_reply.started":"2024-03-30T09:55:35.148772Z","shell.execute_reply":"2024-03-30T09:55:35.153321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen = ImageDataGenerator(preprocessing_function=prefun).flow_from_dataframe(\n    df_train,\n    target_size=(IMG_DIM, IMG_DIM),\n    classes=CLASSES,\n)\n\nval_gen = ImageDataGenerator(preprocessing_function=prefun).flow_from_dataframe(\n    df_val,\n    target_size=(IMG_DIM, IMG_DIM),\n    classes=CLASSES,\n    shuffle=False,\n)\n\ntest_gen = ImageDataGenerator(preprocessing_function=prefun).flow_from_dataframe(\n    df_test,\n    target_size=(IMG_DIM, IMG_DIM),\n    classes=CLASSES,\n    shuffle=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:55:42.148396Z","iopub.execute_input":"2024-03-30T09:55:42.149272Z","iopub.status.idle":"2024-03-30T09:56:47.417896Z","shell.execute_reply.started":"2024-03-30T09:55:42.149239Z","shell.execute_reply":"2024-03-30T09:56:47.417016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen.class_indices","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:56:47.41931Z","iopub.execute_input":"2024-03-30T09:56:47.41957Z","iopub.status.idle":"2024-03-30T09:56:47.425596Z","shell.execute_reply.started":"2024-03-30T09:56:47.419543Z","shell.execute_reply":"2024-03-30T09:56:47.424692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_gen[0][0][0]\n\nimg.max()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:56:47.42734Z","iopub.execute_input":"2024-03-30T09:56:47.428059Z","iopub.status.idle":"2024-03-30T09:56:47.749399Z","shell.execute_reply.started":"2024-03-30T09:56:47.428024Z","shell.execute_reply":"2024-03-30T09:56:47.748442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"### 1. Custom CNN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAvgPool2D, Dense, Flatten\n\nmodel = Sequential()\nmodel.add(Conv2D(16, 7, activation='relu', input_shape=(IMG_DIM, IMG_DIM, 3)))\nmodel.add(Conv2D(16, 7, activation='relu'))\nmodel.add(Conv2D(16, 7, activation='relu'))\nmodel.add(MaxPool2D(2))\nmodel.add(Conv2D(32, 5, activation='relu'))\nmodel.add(Conv2D(32, 5, activation='relu'))\nmodel.add(Conv2D(32, 5, activation='relu'))\nmodel.add(MaxPool2D(2))\nmodel.add(Conv2D(64, 3, activation='relu'))\nmodel.add(Conv2D(64, 3, activation='relu'))\nmodel.add(Conv2D(64, 3, activation='relu', name='last_conv'))\nmodel.add(MaxPool2D(2))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nMODEL_PATH = 'saved-models/cnn.weights.h5'","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:56:50.287949Z","iopub.execute_input":"2024-03-30T09:56:50.288807Z","iopub.status.idle":"2024-03-30T09:56:51.102051Z","shell.execute_reply.started":"2024-03-30T09:56:50.288776Z","shell.execute_reply":"2024-03-30T09:56:51.101064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#basic cnn\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(MODEL_PATH,\n                            save_weights_only=True,\n                            save_best_only=True,\n                            verbose=1)\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=[checkpoint],\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T09:56:57.366732Z","iopub.execute_input":"2024-03-30T09:56:57.367431Z","iopub.status.idle":"2024-03-30T10:05:03.723908Z","shell.execute_reply.started":"2024-03-30T09:56:57.367388Z","shell.execute_reply":"2024-03-30T10:05:03.722979Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. MobileNet","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.layers import GlobalAvgPool2D, Dense","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:05:15.83372Z","iopub.execute_input":"2024-03-30T10:05:15.834411Z","iopub.status.idle":"2024-03-30T10:05:15.842317Z","shell.execute_reply.started":"2024-03-30T10:05:15.834377Z","shell.execute_reply":"2024-03-30T10:05:15.841476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained = MobileNetV2(\n    input_shape=(IMG_DIM, IMG_DIM, 3),\n    include_top=False\n)\npretrained.trainable = False\n\nmodel = Sequential()\nmodel.add(pretrained)\nmodel.add(GlobalAvgPool2D())\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nMODEL_PATH = 'saved-models/mobilenet.weights.h5'","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:05:18.548967Z","iopub.execute_input":"2024-03-30T10:05:18.549781Z","iopub.status.idle":"2024-03-30T10:05:21.463901Z","shell.execute_reply.started":"2024-03-30T10:05:18.549748Z","shell.execute_reply":"2024-03-30T10:05:21.463083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:05:22.723865Z","iopub.execute_input":"2024-03-30T10:05:22.724258Z","iopub.status.idle":"2024-03-30T10:05:22.935811Z","shell.execute_reply.started":"2024-03-30T10:05:22.724228Z","shell.execute_reply":"2024-03-30T10:05:22.934933Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mobileNet\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(MODEL_PATH,\n                            save_weights_only=True,\n                            save_best_only=True,\n                            verbose=1)\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=[checkpoint],\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:05:37.664704Z","iopub.execute_input":"2024-03-30T10:05:37.665343Z","iopub.status.idle":"2024-03-30T10:11:28.923392Z","shell.execute_reply.started":"2024-03-30T10:05:37.66531Z","shell.execute_reply":"2024-03-30T10:11:28.922592Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. ResNet","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet import ResNet152","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:11:28.925055Z","iopub.execute_input":"2024-03-30T10:11:28.925356Z","iopub.status.idle":"2024-03-30T10:11:28.931172Z","shell.execute_reply.started":"2024-03-30T10:11:28.925325Z","shell.execute_reply":"2024-03-30T10:11:28.930307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained = ResNet152(\n    input_shape=(IMG_DIM, IMG_DIM, 3),\n    include_top=False\n)\npretrained.trainable = False\n\nmodel = Sequential()\nmodel.add(pretrained)\nmodel.add(GlobalAvgPool2D())\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nMODEL_PATH = 'saved-models/resnet.weights.h5'","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:11:28.93235Z","iopub.execute_input":"2024-03-30T10:11:28.932696Z","iopub.status.idle":"2024-03-30T10:11:44.643424Z","shell.execute_reply.started":"2024-03-30T10:11:28.932667Z","shell.execute_reply":"2024-03-30T10:11:44.6425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:23:29.105847Z","iopub.execute_input":"2024-03-30T10:23:29.106554Z","iopub.status.idle":"2024-03-30T10:23:29.756021Z","shell.execute_reply.started":"2024-03-30T10:23:29.106521Z","shell.execute_reply":"2024-03-30T10:23:29.755132Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resnet\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(MODEL_PATH,\n                            save_weights_only=True,\n                            save_best_only=True,\n                            verbose=1)\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=[checkpoint],\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:23:32.79168Z","iopub.execute_input":"2024-03-30T10:23:32.792042Z","iopub.status.idle":"2024-03-30T10:32:31.998552Z","shell.execute_reply.started":"2024-03-30T10:23:32.792017Z","shell.execute_reply":"2024-03-30T10:32:31.997764Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import ConvNeXtXLarge","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:32:32.000304Z","iopub.execute_input":"2024-03-30T10:32:32.000567Z","iopub.status.idle":"2024-03-30T10:32:32.004736Z","shell.execute_reply.started":"2024-03-30T10:32:32.000544Z","shell.execute_reply":"2024-03-30T10:32:32.003915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained = ConvNeXtXLarge(\n    input_shape=(IMG_DIM, IMG_DIM, 3),\n    include_top=False\n)\npretrained.trainable = False\n\nmodel = Sequential()\nmodel.add(pretrained)\nmodel.add(GlobalAvgPool2D())\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\nMODEL_PATH = 'saved-models/convnext_xlarge.weights.h5'","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:32:32.005817Z","iopub.execute_input":"2024-03-30T10:32:32.006147Z","iopub.status.idle":"2024-03-30T10:33:39.549936Z","shell.execute_reply.started":"2024-03-30T10:32:32.006118Z","shell.execute_reply":"2024-03-30T10:33:39.549063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-30T10:33:39.551568Z","iopub.execute_input":"2024-03-30T10:33:39.551827Z","iopub.status.idle":"2024-03-30T10:33:39.945051Z","shell.execute_reply.started":"2024-03-30T10:33:39.551804Z","shell.execute_reply":"2024-03-30T10:33:39.944185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ConvNeXtXLarge\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(MODEL_PATH,\n                            save_weights_only=True,\n                            save_best_only=True,\n                            verbose=1)\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=50,\n    callbacks=[checkpoint],\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T10:33:39.946048Z","iopub.execute_input":"2024-03-30T10:33:39.946301Z","iopub.status.idle":"2024-03-30T12:21:54.453074Z","shell.execute_reply.started":"2024-03-30T10:33:39.946278Z","shell.execute_reply":"2024-03-30T12:21:54.451737Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading best weights and Testing","metadata":{}},{"cell_type":"code","source":"print(MODEL_PATH)\nmodel.load_weights(MODEL_PATH)\nmodel.evaluate(test_gen)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:21:59.142685Z","iopub.execute_input":"2024-03-30T12:21:59.143061Z","iopub.status.idle":"2024-03-30T12:23:51.92548Z","shell.execute_reply.started":"2024-03-30T12:21:59.143031Z","shell.execute_reply":"2024-03-30T12:23:51.924441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing model\n### 1. Saliency maps","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef saliency_map(img):\n    \n    plt.imshow(img/255.0)\n    plt.show()\n\n    x = tf.Variable(np.expand_dims(img, 0), dtype='float32')\n\n    with tf.GradientTape() as tape:\n        result = model(x)\n        y = result[:, np.argmax(result)]\n        grads = tape.gradient(y, x)\n\n    grads_2d = grads.numpy()[0].max(axis=2)\n    grads_2d_norm = (grads_2d - grads_2d.min())/(grads_2d.max() - grads_2d.min())\n\n    grads_2d_uint8 = (grads_2d_norm*255.0).astype('uint8')\n    _, grads_2d_bin = cv2.threshold(grads_2d_uint8, 0, 255, cv2.THRESH_OTSU)\n    plt.imshow(grads_2d_bin, cmap='gray')\n    plt.show()\n    \n    print(CLASSES[np.argmax(result)])","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:24:02.778025Z","iopub.execute_input":"2024-03-30T12:24:02.778389Z","iopub.status.idle":"2024-03-30T12:24:02.787801Z","shell.execute_reply.started":"2024-03-30T12:24:02.778359Z","shell.execute_reply":"2024-03-30T12:24:02.786809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = test_gen[0][0][1]\nsaliency_map(img)    ","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:24:44.613297Z","iopub.execute_input":"2024-03-30T12:24:44.613666Z","iopub.status.idle":"2024-03-30T12:24:51.931223Z","shell.execute_reply.started":"2024-03-30T12:24:44.613637Z","shell.execute_reply":"2024-03-30T12:24:51.930286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Gradient-weighted Class Activation Mapping (Grad-CAM)","metadata":{}},{"cell_type":"code","source":"def grad_cam_heatmap(image, last_conv_layer_name='last_conv'):\n    \n    if model.layers[0].__class__.__name__ == 'Functional':\n        last_conv_layer_idx = 0\n        last_conv_layer_model = model.layers[0]\n    else:\n        last_conv_layer = model.get_layer(last_conv_layer_name)\n        last_conv_layer_idx = model.layers.index(last_conv_layer)\n        last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n\n\n    classifier_input = tf.keras.Input(shape=last_conv_layer_model.output.shape[1:])\n    x = classifier_input\n    classifier_layers = model.layers[last_conv_layer_idx+1:]\n    for layer in classifier_layers:\n        x = layer(x)\n    classifier_model = tf.keras.Model(classifier_input, x)\n\n\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(image)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n        print(CLASSES[top_pred_index])\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n\n    return heatmap","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:25:00.945089Z","iopub.execute_input":"2024-03-30T12:25:00.945943Z","iopub.status.idle":"2024-03-30T12:25:00.956959Z","shell.execute_reply.started":"2024-03-30T12:25:00.945906Z","shell.execute_reply":"2024-03-30T12:25:00.955938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.cm as cm\n\ndef grad_cam(img, last_conv_layer_name='last_conv'):\n    colors = cm.jet(np.arange(256))[:, :3]\n    gc_mask = grad_cam_heatmap(np.expand_dims(img, 0), last_conv_layer_name)\n    gc_mask_uint8 = (gc_mask*255.0).astype('uint8')\n    heatmap = colors[gc_mask_uint8]\n    heatmap = cv2.resize(heatmap, (IMG_DIM, IMG_DIM))\n    heatmap = (heatmap*255).astype('uint8')\n    img_uint8 = img.astype('uint8')\n    img_overlay = cv2.addWeighted(src1=img_uint8, alpha=0.6, src2=heatmap, beta=0.4, gamma=0.0)\n    plt.imshow(img_overlay)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:25:09.288682Z","iopub.execute_input":"2024-03-30T12:25:09.289429Z","iopub.status.idle":"2024-03-30T12:25:09.29626Z","shell.execute_reply.started":"2024-03-30T12:25:09.289397Z","shell.execute_reply":"2024-03-30T12:25:09.29534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = test_gen[20][0][4]\ngrad_cam(img)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:25:30.7906Z","iopub.execute_input":"2024-03-30T12:25:30.791588Z","iopub.status.idle":"2024-03-30T12:25:37.991014Z","shell.execute_reply.started":"2024-03-30T12:25:30.791553Z","shell.execute_reply":"2024-03-30T12:25:37.990008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}